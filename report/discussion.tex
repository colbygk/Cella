\section{Discussion} \label{discussion}

There are a number of interesting observations regarding the results reported above; here we survey these 
conclusions.

\subsection{On the difference between radius 2 and 3}

As noted above, the size of the search space grows with the radius of a rule ($2^{32}$ for $r = 2$, and $2^{128}$ for $r = 3$), suggesting 
that searches may be more difficult for CAs with larger neighborhoods. However, a larger radius also means that the CA has more information 
to work with in deciding which state to transition to, suggesting that finding a solution may be easier for larger radii CAs despite the increased 
search size.

Figures~\ref{fig:r2_rho} -~\ref{fig:r3_best_fit} show that ??????????

As noted above, an early strategy for increasing fitness is simply to go to $\rho_n = 0.0$ or $\rho_n = 1.0$. An interesting outcome for both 
cases was the presence of fit rules that adopted a different version of the same sort of 'gambling' strategy: Go to $\rho_n = 0.0$ or 
$\rho = 1.0$ but instead of entering a stable state, oscillate between the two. Whether the classification is evaluated as correct then 
depends on which global state the CA happens to be in after 200 iterations.

\subsection{On mutational robustness}

Here we discuss 2.2.

\subsection{On the quest for a better solution}

Here we discuss 2.3.


